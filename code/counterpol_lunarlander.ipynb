{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counterpol on LunarLander-v2\n",
    "\n",
    "This notebook shows how to estimate counterfactuals for A2C policies on LunarLander-v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3 import A2C\n",
    "from torch.distributions import Categorical\n",
    "from stable_baselines3.common.policies import obs_as_tensor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.policies import obs_as_tensor\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading A2C policy\n",
    "def load_saved_model(env, model_id, save_path, name_prefix):\n",
    "    model = A2C.load(os.path.join(save_path, name_prefix + \"_\" + str(model_id) + \"_steps\"), env=env)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_action(policy_network, state):\n",
    "    '''\n",
    "    Function to sample action from policy network at a given state\n",
    "    '''\n",
    "    state = state.view(1, -1)\n",
    "    probs = policy_network.get_distribution(state).distribution.probs\n",
    "    probs = probs.squeeze(0)\n",
    "    action = probs.argmax()\n",
    "    return action.item(), probs[action], torch.log(probs[action])\n",
    "\n",
    "\n",
    "def evaluate_performance(collected_episodes):\n",
    "    '''\n",
    "    Function to evaluate J_pi from trajectories collected using policy pi\n",
    "    '''\n",
    "    episodic_returns = []\n",
    "    for episode in collected_episodes:\n",
    "        episodic_returns.append(sum([transition[3] for transition in episode]))\n",
    "    J_pi = np.mean(episodic_returns)\n",
    "    return J_pi\n",
    "\n",
    "\n",
    "def compute_stepwise_returns(collected_episodes):\n",
    "    '''\n",
    "    Returns Monte Carlo estimate of Q_pi(s,a) for each step in each episode\n",
    "    '''\n",
    "    per_step_returns = []\n",
    "    for episode in collected_episodes:\n",
    "        per_step_returns_ = []\n",
    "        for transition in episode[::-1]:\n",
    "            per_step_returns_.append(\n",
    "                transition[3] + (per_step_returns_[-1] if len(per_step_returns_) > 0 else 0))\n",
    "        per_step_returns.append(per_step_returns_[::-1])\n",
    "    return per_step_returns\n",
    "\n",
    "\n",
    "def rollout_policy(env, policy_network, device, num_rollouts=10, max_rollout_length=100):\n",
    "    '''\n",
    "    Collects trajectories using policy pi\n",
    "    '''\n",
    "    collected_episodes = []\n",
    "    total_steps = 0\n",
    "\n",
    "    for rollout_id in range(num_rollouts):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        step_id = 0\n",
    "\n",
    "        while True:\n",
    "            if type(state) == int:\n",
    "                state = np.array([state])\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "            action, _, log_action_prob = get_action(policy_network, state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, log_action_prob, reward, next_state, done))\n",
    "            state = next_state\n",
    "            step_id += 1\n",
    "\n",
    "            if done or step_id >= max_rollout_length:\n",
    "                collected_episodes.append(episode)\n",
    "                total_steps += step_id\n",
    "                break\n",
    "\n",
    "    return collected_episodes, total_steps\n",
    "\n",
    "\n",
    "def calculate_counterfactual_objective(env, policy_network_orig, policy_network_cf, target_return, device, kl_weight=0.1, num_rollouts=10, max_rollout_length=100, reward_range=200):\n",
    "    '''\n",
    "    Computing Counterpol objective function\n",
    "    '''\n",
    "\n",
    "    collected_episodes, total_steps = rollout_policy(env, policy_network_cf, device, num_rollouts, max_rollout_length)\n",
    "    per_step_returns = compute_stepwise_returns(collected_episodes)\n",
    "    J_pi = evaluate_performance(collected_episodes)\n",
    "    objective = 0\n",
    "    kl_objective = 0\n",
    "    performance_objective_orig = (J_pi - target_return)**2 / (reward_range**2)\n",
    "\n",
    "    for episode_id, episode in enumerate(collected_episodes):\n",
    "        for step_id, (state, action, log_action_prob, reward, next_state, done) in enumerate(episode):\n",
    "            _, pi_orig_action, _ = get_action(policy_network_orig, state)\n",
    "            kl_objective = (-1.0 * pi_orig_action.detach() * log_action_prob) / total_steps\n",
    "            performance_objective = 2 * (J_pi - target_return) * (per_step_returns[episode_id][step_id] - target_return) * log_action_prob / (reward_range**2 * total_steps)\n",
    "            objective += (kl_weight * kl_objective + (1 - kl_weight) * performance_objective)\n",
    "\n",
    "    return objective, kl_objective, performance_objective_orig, J_pi\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Counterpol Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_returns = [100, 150, 0, -50]\n",
    "\n",
    "for target_return in target_returns:\n",
    "    config = {\n",
    "        'env': 'LunarLander-v2',\n",
    "        'learning_rate': 0.0005,\n",
    "        'kl_weight': 0.9,\n",
    "        'num_rollouts': 10,\n",
    "        'model_id': 100000,\n",
    "        'num_pol_iters': 10,\n",
    "        'max_cf_iters': 250,\n",
    "        'max_rollout_length': 500,\n",
    "        'reward_range': 500,\n",
    "        'target_return': target_return,\n",
    "        'delta_return': 5,\n",
    "        'grad_clip_value': 0.5,\n",
    "        'pretrained_save_path': './orig_ckpts/',\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "    # Set seeds\n",
    "    seed = config['seed']\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create environment\n",
    "    env = gym.make(config['env'])\n",
    "\n",
    "    save_path = config['pretrained_save_path']\n",
    "    name_prefix = config['env'].lower()\n",
    "\n",
    "    # Logging\n",
    "    wandb.init(\n",
    "        project=\"RL-Factuals\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Load pretrained model\n",
    "    model_id = config['model_id']\n",
    "    model = load_saved_model(env, model_id, save_path, name_prefix)\n",
    "\n",
    "    # Counterpol starts -->\n",
    "\n",
    "    # Create copy of original policy network\n",
    "    policy_network_orig = model.policy\n",
    "    policy_network_cf_0 = deepcopy(policy_network_orig)\n",
    "    policy_network_cf = deepcopy(policy_network_orig)\n",
    "\n",
    "    # Set up optimizer\n",
    "    optimizer = torch.optim.Adam(\n",
    "        policy_network_cf.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # Number of policy iterations\n",
    "    num_pol_iters = config['num_pol_iters']\n",
    "    max_cf_iters = config['max_cf_iters']\n",
    "\n",
    "    found_counterfactual = False\n",
    "\n",
    "    # Outerloop\n",
    "    for _ in tqdm(range(num_pol_iters)):\n",
    "        \n",
    "        # Innerloop\n",
    "        for _ in tqdm(range(max_cf_iters)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            cf_objective, kl_objective, performance_objective_orig, J_pi =\\\n",
    "                calculate_counterfactual_objective(env,\n",
    "                                                    policy_network_cf_0,\n",
    "                                                    policy_network_cf,\n",
    "                                                    target_return=config['target_return'],\n",
    "                                                    device=device,\n",
    "                                                    kl_weight=config['kl_weight'],\n",
    "                                                    num_rollouts=config['num_rollouts'],\n",
    "                                                    max_rollout_length=config['max_rollout_length'],\n",
    "                                                    reward_range=config['reward_range'])\n",
    "\n",
    "            wandb.log({'counterfactual_objective': kl_objective+performance_objective_orig,\n",
    "                        'J_pi': J_pi,\n",
    "                        'kl_objective': kl_objective,\n",
    "                        'performance_objective_orig': performance_objective_orig})\n",
    "\n",
    "            # Check if counterfactual found\n",
    "            if abs(J_pi - config['target_return']) < config['delta_return']:\n",
    "                print(\"Found counterfactual\")\n",
    "                found_counterfactual = True\n",
    "                # Break innerloop\n",
    "                break\n",
    "            else:\n",
    "                cf_objective.backward()                \n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    policy_network_cf.parameters(), config['grad_clip_value'])\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "        # Break outerloop\n",
    "        if found_counterfactual:\n",
    "            break\n",
    "        \n",
    "        # Change KL-pivot\n",
    "        with torch.no_grad():\n",
    "            policy_network_cf_0.load_state_dict(policy_network_cf.state_dict())\n",
    "\n",
    "    # Evaluate the counterfactual policy\n",
    "    model.policy = policy_network_cf\n",
    "    mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20)\n",
    "    print(f\"Counterfactual policy J_pi:{mean_reward:.2f} +/- {std_reward}\")\n",
    "\n",
    "    # Save counterfactual policy\n",
    "    model.save(f\"./logs/lunarlander/cf_orig_id_{model_id}_target_return_{target_return}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Original Policy with Counterfactual Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "video_folder = './logs/videos/'\n",
    "video_length = 1000\n",
    "\n",
    "# Load original model\n",
    "model_id = config['model_id']\n",
    "orig_model = load_saved_model(env, model_id, save_path, name_prefix)\n",
    "\n",
    "env = DummyVecEnv([lambda: gym.make(config['env'])])\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Record the video starting at the first step\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
    "                       name_prefix=\"orig-{}\".format(config['env']))\n",
    "\n",
    "env.reset()\n",
    "for _ in range(video_length + 1):\n",
    "  action = orig_model.predict(obs)[0]\n",
    "  obs, _, done, _ = env.step(action)\n",
    "  if done:\n",
    "    break\n",
    "# Save the video\n",
    "env.close()\n",
    "\n",
    "\n",
    "# Note the counterfactual policy is saved as model in the previous cell\n",
    "env = DummyVecEnv([lambda: gym.make(config['env'])])\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "# Record the video starting at the first step\n",
    "env = VecVideoRecorder(env, video_folder,\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=video_length,\n",
    "                       name_prefix=f\"counterfactual-target-return-{config['target_return']}-{config['env']}\")\n",
    "\n",
    "env.reset()\n",
    "for _ in range(video_length + 1):\n",
    "  action = model.predict(obs)[0]\n",
    "  obs, _, _, done = env.step(action)\n",
    "  if done:\n",
    "    break\n",
    "# Save the video\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "counterpol",
   "language": "python",
   "name": "counterpol"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
